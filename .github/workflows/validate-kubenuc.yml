name: kubenuc-full-cluster-e2e

on:
  pull_request:
    branches: [main]
    paths:
      - 'clusters/kubenuc/**'

jobs:
  cluster-test:
    runs-on: self-hosted
    timeout-minutes: 120
    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '>=3.11'

      - name: Setup Flux
        uses: fluxcd/flux2/action@main
        with:
          version: 2.7.5

      - name: Setup k3s
        run: |
          curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--disable=traefik --write-kubeconfig-mode=644" sh -
          mkdir -p ~/.kube
          sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
          sudo chown $(id -u):$(id -g) ~/.kube/config
          kubectl wait --for=condition=Ready nodes --all --timeout=120s

      - name: Configure k3s registry mirrors
        run: |
          sudo mkdir -p /etc/rancher/k3s
          sudo tee /etc/rancher/k3s/registries.yaml <<'EOF'
          mirrors:
            "docker.io":
              endpoint:
                - "https://mirror.gcr.io"
                - "https://harbor.ddlns.net/v2/dolibarr/"
            "registry-1.docker.io":
              endpoint:
                - "https://mirror.gcr.io"
                - "https://harbor.ddlns.net/v2/dolibarr/"
          EOF
          sudo systemctl restart k3s
          kubectl wait --for=condition=Ready nodes --all --timeout=120s

      - name: Install Flux in Kubernetes
        run: flux install --timeout=5m
          
      - name: Verify Flux installation
        run: |
          kubectl -n flux-system wait --for=condition=ready pod --all --timeout=5m
          kubectl -n flux-system get deploy
          kubectl get crds | grep -E "(fluxcd|toolkit)" || true
          kubectl get crd helmreleases.helm.toolkit.fluxcd.io || true

      - name: Install cert-manager
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.0/cert-manager.yaml
          kubectl -n cert-manager wait --for=condition=ready pod --all --timeout=5m

      - name: Create test ClusterIssuer
        run: |
          kubectl apply -f - <<EOF
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt
          spec:
            selfSigned: {}
          EOF

      - name: Deploy 1password
        run: |
          kubectl create ns 1password || true
          kubectl create secret generic onepassword-secret --from-literal=token=${{ secrets.OP_TOKEN }} --namespace=1password || true

      - name: Main branch setup
        run: |
          flux create source git flux-system \
          --url=${{ github.event.repository.html_url }} \
          --branch=main \
          --interval=15m \
          --username=${GITHUB_ACTOR} \
          --password=${{ secrets.GITHUB_TOKEN }} \
          --ignore-paths="clusters/kubenuc-test/flux-system/**" \
          --timeout=60m

          flux create kustomization flux-system \
          --source=flux-system \
          --interval=15m \
          --path=./clusters/kubenuc-test \
          --timeout=60m

      - name: Wait for initial reconciliation
        run: |
          kubectl wait --for=condition=ready pod -l app=source-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=kustomize-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=helm-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=notification-controller -n flux-system --timeout=5m || true
          kubectl -n flux-system wait kustomization/charts --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/apps --for=condition=ready --timeout=30m || true
          echo "Waiting for HR to reconcile..."
          kubectl get hr -A --no-headers | awk '{print $1, $2}' | while read ns name; do
              echo "Waiting for HR: $name"
              kubectl -n "$ns" wait hr "$name" --for=condition=ready --timeout=30m
          done

      - name: Apply feature branch changes
        run: |
          CURRENT_BRANCH="${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}"
          kubectl patch gitrepository flux-system -n flux-system \
            --type merge \
            --patch "{\"spec\":{\"ref\":{\"branch\":\"${CURRENT_BRANCH}\"}}}"
          kubectl wait --for=condition=ready gitrepository/flux-system -n flux-system --timeout=30m

      - name: Verify feature branch reconciliation
        run: |
          flux reconcile source git flux-system --timeout=60m || true
          kubectl -n flux-system wait kustomization/flux-system --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/charts --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/apps --for=condition=ready --timeout=30m || true
          sleep 120
          kubectl get hr -A --no-headers | awk '{print $1, $2}' | while read ns name; do
              echo "Waiting for HR: $name"
              kubectl -n "$ns" wait hr "$name" --for=condition=ready --timeout=30m
          done

      - name: Configure CoreDNS and hosts for test domains
        run: |
          echo "=== Configuring CoreDNS for test domains ==="

          DEFAULT_INGRESS_CLASS=$(kubectl get ingressclass -o jsonpath='{.items[?(@.metadata.annotations.ingressclass\.kubernetes\.io/is-default-class=="true")].metadata.name}' 2>/dev/null || echo "")

          if [ -n "$DEFAULT_INGRESS_CLASS" ]; then
            echo "Found default IngressClass: $DEFAULT_INGRESS_CLASS"
          fi

          INGRESS_IP=""

          for NS in ingress-nginx ingress emissary-ingress traefik haproxy-ingress kong; do
            if kubectl get ns "$NS" &>/dev/null; then
              SVC=$(kubectl -n "$NS" get svc -o json | jq -r '
                .items[] |
                select(
                  (.spec.type == "LoadBalancer" or .spec.type == "ClusterIP") and
                  (
                    .metadata.labels["app.kubernetes.io/component"] == "controller" or
                    .metadata.labels["app"] == "controller" or
                    .metadata.name | contains("controller") or
                    .metadata.name | contains("ingress")
                  )
                ) |
                "\(.metadata.name) \(.spec.clusterIP)"
              ' | head -1)

              if [ -n "$SVC" ]; then
                SVC_NAME=$(echo "$SVC" | awk '{print $1}')
                INGRESS_IP=$(echo "$SVC" | awk '{print $2}')
                echo "Found ingress service: $SVC_NAME in namespace $NS with IP $INGRESS_IP"
                break
              fi
            fi
          done

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ]; then
            echo "Trying fallback: looking for any service exposing port 80/443..."
            INGRESS_IP=$(kubectl get svc -A -o json | jq -r '
              .items[] |
              select(
                .spec.ports[]? |
                select(.port == 80 or .port == 443 or .targetPort == 80 or .targetPort == 443)
              ) |
              select(.metadata.namespace != "default" and .metadata.namespace != "kube-system") |
              .spec.clusterIP
            ' | grep -v null | head -1)
          fi

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ] || [ "$INGRESS_IP" == "None" ]; then
            echo "Using k3s node IP as fallback..."
            INGRESS_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
          fi

          echo "Final Ingress IP: $INGRESS_IP"

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ]; then
            echo "Could not determine ingress IP, using placeholder"
            INGRESS_IP="10.96.0.100"
          fi

          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: coredns-custom
            namespace: kube-system
          data:
            tst.ddlns.net.override: |
              ${INGRESS_IP} harbor.tst.ddlns.net
              ${INGRESS_IP} jenkins.tst.ddlns.net
              ${INGRESS_IP} artifactory.tst.ddlns.net
              ${INGRESS_IP} cloud.tst.ddlns.net
              ${INGRESS_IP} portainer.tst.ddlns.net
              ${INGRESS_IP} tv.tst.ddlns.net
              ${INGRESS_IP} sso.tst.ddlns.net
              ${INGRESS_IP} s3-api.tst.ddlns.net
              ${INGRESS_IP} nx.s3.tst.ddlns.net
          EOF

          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: coredns
            namespace: kube-system
          data:
            Corefile: |
              .:53 {
                  errors
                  health {
                      lameduck 5s
                  }
                  ready
                  kubernetes cluster.local in-addr.arpa ip6.arpa {
                      pods insecure
                      fallthrough in-addr.arpa ip6.arpa
                      ttl 30
                  }
                  hosts /etc/coredns/custom/tst.ddlns.net.override {
                      fallthrough
                  }
                  prometheus :9153
                  forward . /etc/resolv.conf {
                      max_concurrent 1000
                  }
                  cache 30
                  loop
                  reload
                  loadbalance
              }
          EOF
          kubectl -n kube-system patch deployment coredns --type=strategic -p '{"spec":{"template":{"spec":{"volumes":[{"name":"custom-config","configMap":{"name":"coredns-custom","optional":true}}],"containers":[{"name":"coredns","volumeMounts":[{"name":"custom-config","mountPath":"/etc/coredns/custom","readOnly":true}]}]}}}}'
          kubectl -n kube-system rollout restart deployment coredns
          kubectl -n kube-system rollout status deployment coredns --timeout=10m

      - name: Check resource deployment status
        run: |
          flux get kustomizations -A
          flux get helmreleases -A

      - name: Validate critical deployments
        run: |
          kubectl -n ingress-nginx get pods || true
          kubectl get ingress -A || true
          kubectl -n openebs get pods || true
          kubectl get certificates -A || true
          kubectl get sc || true

      - name: Check for failed reconciliations
        run: |
          kubectl get kustomizations -A -o json | jq -r '.items[] | select(.status.conditions[]? | select(.type=="Ready" and .status=="False")) | "\(.metadata.namespace)/\(.metadata.name)"' || true
          kubectl get helmreleases -A -o json | jq -r '.items[] | select(.status.conditions[]? | select(.type=="Ready" and .status=="False")) | "\(.metadata.namespace)/\(.metadata.name)"' || true

      - name: Run Robot Framework tests in Kubernetes
        id: robot
        run: |
          echo "=== Deploying Robot Framework tests as Kubernetes Job ==="
          kubectl apply -f tests/robot-job.yaml

          echo "Waiting for Job to complete (timeout: 10m)..."
          kubectl wait --for=condition=complete job/robot-tests --timeout=600s || {
            echo "Job did not complete successfully"
            kubectl logs job/robot-tests || true
            kubectl describe job/robot-tests || true
          }

          JOB_STATUS=$(kubectl get job robot-tests -o jsonpath='{.status.succeeded}')
          if [ "$JOB_STATUS" == "1" ]; then
            echo "ROBOT_RESULT=success" >> $GITHUB_ENV
          else
            echo "ROBOT_RESULT=failure" >> $GITHUB_ENV
          fi

      - name: Extract Robot Framework results from pod
        if: always()
        run: |
          mkdir -p results
          POD_NAME=$(kubectl get pods -l job-name=robot-tests -o jsonpath='{.items[0].metadata.name}')
          if [ -n "$POD_NAME" ]; then
            echo "Copying results from pod $POD_NAME..."
            kubectl cp default/${POD_NAME}:/results/. results/ || {
              echo "Failed to copy results, fetching logs instead..."
              kubectl logs $POD_NAME > results/pod-logs.txt || true
            }
          fi
          ls -la results/ || true

      - name: Upload Robot Framework results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: robot-framework-results
          path: results/
          retention-days: 7

      - name: Parse Robot Framework results
        if: always()
        id: parse_results
        run: |
          if [ -f results/output.xml ]; then
            PASS=$(grep -oP 'pass="\K[0-9]+' results/output.xml | head -1 || echo "0")
            FAIL=$(grep -oP 'fail="\K[0-9]+' results/output.xml | head -1 || echo "0")
            echo "pass=${PASS}" >> $GITHUB_OUTPUT
            echo "fail=${FAIL}" >> $GITHUB_OUTPUT
          else
            echo "pass=0" >> $GITHUB_OUTPUT
            echo "fail=0" >> $GITHUB_OUTPUT
          fi

      - name: Comment PR with Robot Framework results
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const pass = '${{ steps.parse_results.outputs.pass }}';
            const fail = '${{ steps.parse_results.outputs.fail }}';
            const total = parseInt(pass) + parseInt(fail);
            const status = fail === '0' && total > 0 ? '✅ All tests passed' : '❌ Some tests failed';
            const artifactUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;

            let screenshotSection = '';
            const screenshotDir = 'results/screenshots';
            if (fs.existsSync(screenshotDir)) {
              const files = fs.readdirSync(screenshotDir).filter(f => f.endsWith('.png'));
              if (files.length > 0) {
                screenshotSection = '\n\n<details>\n<summary>Screenshots</summary>\n\n';
                screenshotSection += 'Screenshots are available in the [workflow artifacts](' + artifactUrl + ').\n\n';
                screenshotSection += '| Service | Status |\n|---------|--------|\n';
                files.forEach(f => {
                  const service = f.replace('.png', '');
                  screenshotSection += `| ${service} | Captured |\n`;
                });
                screenshotSection += '\n</details>';
              }
            }

            const body = `## Robot Framework Test Results

            ${status}

            | Metric | Count |
            |--------|-------|
            | Passed | ${pass} |
            | Failed | ${fail} |
            | Total | ${total} |

            [View full report and screenshots](${artifactUrl})${screenshotSection}`;

            github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });

      - name: Cleanup Robot Framework Job
        if: always()
        run: |
          kubectl delete job robot-tests --ignore-not-found=true
          kubectl delete configmap robot-tests --ignore-not-found=true

      - name: Debug failure
        if: failure()
        run: |
          flux get all -A
          kubectl get pods -A | grep -v Running || true
          kubectl get events -A --sort-by='.lastTimestamp' | tail -30 || true

      - name: Cleanup k3s
        if: always()
        run: |
          /usr/local/bin/k3s-uninstall.sh || true
          rm -rf ~/.kube/config || true
          sudo rm -rf /etc/rancher/k3s || true
          sudo rm -rf /var/lib/rancher/k3s || true
          sudo rm -rf /run/k3s || true

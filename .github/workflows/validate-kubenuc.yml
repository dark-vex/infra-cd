name: kubenuc-full-cluster-e2e

on:
  pull_request:
    branches: [main]
    paths:
      - 'clusters/kubenuc/**'

jobs:
  cluster-test:
    runs-on: self-hosted
    timeout-minutes: 120
    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '>=3.11'

      - name: Install Robot Framework and dependencies
        run: |
          pip install --upgrade pip
          pip install robotframework==7.0
          pip install robotframework-requests==0.9.7
          pip install robotframework-jsonlibrary==0.5
          robot --version || true

      - name: Setup Flux
        uses: fluxcd/flux2/action@main
        with:
          version: 2.7.5

      - name: Setup k3s
        run: |
          curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--disable=traefik --write-kubeconfig-mode=644" sh -
          mkdir -p ~/.kube
          sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
          sudo chown $(id -u):$(id -g) ~/.kube/config
          for i in {1..10}; do
            if kubectl version; then
              break
            fi
            echo "Waiting for k3s API server to be ready..."
            sleep 10
          done
          kubectl wait --for=condition=Ready nodes --all --timeout=120s

      - name: Configure k3s registry mirrors
        run: |
          sudo mkdir -p /etc/rancher/k3s
          sudo tee /etc/rancher/k3s/registries.yaml <<'EOF'
          mirrors:
            "docker.io":
              endpoint:
                - "https://mirror.gcr.io"
                - "https://harbor.ddlns.net/v2/dolibarr/"
            "registry-1.docker.io":
              endpoint:
                - "https://mirror.gcr.io"
                - "https://harbor.ddlns.net/v2/dolibarr/"
          EOF
          sudo systemctl restart k3s
          kubectl wait --for=condition=Ready nodes --all --timeout=120s

      - name: Install Flux in Kubernetes
        run: flux install --timeout=5m
          
      - name: Verify Flux installation
        run: |
          kubectl -n flux-system wait --for=condition=ready pod --all --timeout=5m
          kubectl -n flux-system get deploy
          kubectl get crds | grep -E "(fluxcd|toolkit)" || true
          kubectl get crd helmreleases.helm.toolkit.fluxcd.io || true

      - name: Install cert-manager
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.0/cert-manager.yaml
          kubectl -n cert-manager wait --for=condition=ready pod --all --timeout=5m

      - name: Create test ClusterIssuer
        run: |
          kubectl apply -f - <<EOF
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt
          spec:
            selfSigned: {}
          EOF

      - name: Deploy 1password
        run: |
          kubectl create ns 1password || true
          kubectl create secret generic onepassword-secret --from-literal=token=${{ secrets.OP_TOKEN }} --namespace=1password || true

      - name: Main branch setup
        run: |
          flux create source git flux-system \
          --url=${{ github.event.repository.html_url }} \
          --branch=main \
          --interval=15m \
          --username=${GITHUB_ACTOR} \
          --password=${{ secrets.GITHUB_TOKEN }} \
          --ignore-paths="clusters/kubenuc-test/flux-system/**" \
          --timeout=60m

          flux create kustomization flux-system \
          --source=flux-system \
          --interval=15m \
          --path=./clusters/kubenuc-test \
          --timeout=60m

      - name: Wait for initial reconciliation
        run: |
          kubectl wait --for=condition=ready pod -l app=source-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=kustomize-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=helm-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=notification-controller -n flux-system --timeout=5m || true
          kubectl -n flux-system wait kustomization/charts --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/apps --for=condition=ready --timeout=30m || true
          echo "Waiting for HR to reconcile..."
          kubectl get hr -A --no-headers | awk '{print $1, $2}' | while read ns name; do
              echo "Waiting for HR: $name"
              kubectl -n "$ns" wait hr "$name" --for=condition=ready --timeout=30m
          done

      - name: Apply feature branch changes
        run: |
          CURRENT_BRANCH="${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}"
          kubectl patch gitrepository flux-system -n flux-system \
            --type merge \
            --patch "{\"spec\":{\"ref\":{\"branch\":\"${CURRENT_BRANCH}\"}}}"
          kubectl wait --for=condition=ready gitrepository/flux-system -n flux-system --timeout=30m

      - name: Verify feature branch reconciliation
        run: |
          flux reconcile source git flux-system --timeout=60m || true
          kubectl -n flux-system wait kustomization/flux-system --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/charts --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/apps --for=condition=ready --timeout=30m || true
          sleep 120
          kubectl get hr -A --no-headers | awk '{print $1, $2}' | while read ns name; do
              echo "Waiting for HR: $name"
              kubectl -n "$ns" wait hr "$name" --for=condition=ready --timeout=30m
          done

      - name: Configure CoreDNS for test domains
        run: |
          echo "=== Configuring CoreDNS for test domains ==="

          DEFAULT_INGRESS_CLASS=$(kubectl get ingressclass -o jsonpath='{.items[?(@.metadata.annotations.ingressclass\.kubernetes\.io/is-default-class=="true")].metadata.name}' 2>/dev/null || echo "")

          if [ -n "$DEFAULT_INGRESS_CLASS" ]; then
            echo "Found default IngressClass: $DEFAULT_INGRESS_CLASS"
          fi

          INGRESS_IP=""

          for NS in ingress-nginx ingress emissary-ingress traefik haproxy-ingress kong; do
            if kubectl get ns "$NS" &>/dev/null; then
              SVC=$(kubectl -n "$NS" get svc -o json | jq -r '
                .items[] | 
                select(
                  (.spec.type == "LoadBalancer" or .spec.type == "ClusterIP") and
                  (
                    .metadata.labels["app.kubernetes.io/component"] == "controller" or
                    .metadata.labels["app"] == "controller" or
                    .metadata.name | contains("controller") or
                    .metadata.name | contains("ingress")
                  )
                ) | 
                "\(.metadata.name) \(.spec.clusterIP)"
              ' | head -1)
              
              if [ -n "$SVC" ]; then
                SVC_NAME=$(echo "$SVC" | awk '{print $1}')
                INGRESS_IP=$(echo "$SVC" | awk '{print $2}')
                echo "Found ingress service: $SVC_NAME in namespace $NS with IP $INGRESS_IP"
                break
              fi
            fi
          done

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ]; then
            echo "Trying fallback: looking for any service exposing port 80/443..."
            INGRESS_IP=$(kubectl get svc -A -o json | jq -r '
              .items[] | 
              select(
                .spec.ports[]? | 
                select(.port == 80 or .port == 443 or .targetPort == 80 or .targetPort == 443)
              ) |
              select(.metadata.namespace != "default" and .metadata.namespace != "kube-system") |
              .spec.clusterIP
            ' | grep -v null | head -1)
          fi

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ] || [ "$INGRESS_IP" == "None" ]; then
            echo "Using k3d node IP as fallback..."
            INGRESS_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
          fi

          echo "Final Ingress IP: $INGRESS_IP"

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ]; then
            echo "⚠️ Could not determine ingress IP, using placeholder"
            INGRESS_IP="10.96.0.100"
          fi

          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: coredns-custom
            namespace: kube-system
          data:
            tst.ddlns.net.override: |
              ${INGRESS_IP} harbor.tst.ddlns.net
              ${INGRESS_IP} jenkins.tst.ddlns.net
              ${INGRESS_IP} artifactory.tst.ddlns.net
              ${INGRESS_IP} cloud.tst.ddlns.net
              ${INGRESS_IP} portainer.tst.ddlns.net
              ${INGRESS_IP} tv.tst.ddlns.net
              ${INGRESS_IP} sso.tst.ddlns.net
              ${INGRESS_IP} s3-api.tst.ddlns.net
              ${INGRESS_IP} nx.s3.tst.ddlns.net
          EOF

          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: coredns
            namespace: kube-system
          data:
            Corefile: |
              .:53 {
                  errors
                  health {
                      lameduck 5s
                  }
                  ready
                  kubernetes cluster.local in-addr.arpa ip6.arpa {
                      pods insecure
                      fallthrough in-addr.arpa ip6.arpa
                      ttl 30
                  }
                  hosts /etc/coredns/custom/tst.ddlns.net.override {
                      fallthrough
                  }
                  prometheus :9153
                  forward . /etc/resolv.conf {
                      max_concurrent 1000
                  }
                  cache 30
                  loop
                  reload
                  loadbalance
              }
          EOF
          kubectl -n kube-system patch deployment coredns --type=strategic -p '{"spec":{"template":{"spec":{"volumes":[{"name":"custom-config","configMap":{"name":"coredns-custom","optional":true}}],"containers":[{"name":"coredns","volumeMounts":[{"name":"custom-config","mountPath":"/etc/coredns/custom","readOnly":true}]}]}}}}'
          kubectl -n kube-system rollout restart deployment coredns
          kubectl -n kube-system rollout status deployment coredns --timeout=10m

      - name: Check resource deployment status
        run: |
          flux get kustomizations -A
          flux get helmreleases -A

      - name: Validate critical deployments
        run: |
          kubectl -n ingress-nginx get pods || true
          kubectl get ingress -A || true
          kubectl -n openebs get pods || true
          kubectl get certificates -A || true
          kubectl get sc || true

      - name: Check for failed reconciliations
        run: |
          kubectl get kustomizations -A -o json | jq -r '.items[] | select(.status.conditions[]? | select(.type=="Ready" and .status=="False")) | "\(.metadata.namespace)/\(.metadata.name)"' || true
          kubectl get helmreleases -A -o json | jq -r '.items[] | select(.status.conditions[]? | select(.type=="Ready" and .status=="False")) | "\(.metadata.namespace)/\(.metadata.name)"' || true

      - name: Cleanup k3s
        if: always()
        run: |
          /usr/local/bin/k3s-uninstall.sh || true

      - name: Debug failure
        if: failure()
        run: |
          flux get all -A
          kubectl get pods -A | grep -v Running || true
          kubectl get events -A --sort-by='.lastTimestamp' | tail -30 || true




name: kubenuc-full-cluster-e2e

on:
  pull_request:
    branches: [main]
    paths:
      - 'clusters/kubenuc/**'

jobs:
  cluster-test:
    runs-on: self-hosted
    timeout-minutes: 120
    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '>=3.11'

      - name: Setup Flux
        uses: fluxcd/flux2/action@main
        with:
          version: 2.7.5

      - name: Setup k3s
        run: |
          set -euo pipefail
      
          # Install k3s
          curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--disable=traefik --write-kubeconfig-mode=644" INSTALL_K3S_VERSION="v1.33.3+k3s1" sh -
          mkdir -p ~/.kube
          sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
          sudo chown $(id -u):$(id -g) ~/.kube/config
      
          # Helper: wait for systemd unit to become active
          wait_for_unit() {
            local unit="$1"
            local timeout="${2:-120}" # seconds
            local interval=5
            local waited=0
      
            if ! command -v systemctl >/dev/null 2>&1; then
              echo "systemctl not present on runner; cannot check systemd unit $unit"
              return 2
            fi
      
            echo "Checking systemd unit: $unit"
      
            # Ensure unit file exists in systemd
            if ! sudo systemctl list-unit-files | grep -q "^${unit}\."; then
              echo "Unit ${unit} not found in systemd unit-files. Continuing to poll in case it appears..."
            fi
      
            while [ $waited -lt "$timeout" ]; do
              # Check whether the unit is active
              if sudo systemctl is-active --quiet "$unit"; then
                echo "Unit ${unit} is active"
                # Also make sure it's not in a failed state
                if sudo systemctl is-failed --quiet "$unit"; then
                  echo "Unit ${unit} is in failed state"
                  return 3
                fi
                return 0
              fi
      
              # Show transient status periodically to aid debugging
              echo "[$((waited + interval))/${timeout}] unit ${unit} not active yet. status:"
              sudo systemctl status "$unit" --no-pager -n 3 || true
      
              sleep $interval
              waited=$((waited + interval))
            done
      
            echo "Timed out waiting for ${unit} to become active (timeout=${timeout}s)."
            echo "Last 200 journal lines for ${unit}:"
            sudo journalctl -u "$unit" --no-pager -n 200 || true
            return 1
          }
      
          # Wait for k3s systemd unit to be active (returns non-zero on failure)
          wait_for_unit k3s 180
      
          # Optional: ensure enabled
          if command -v systemctl >/dev/null 2>&1; then
            if sudo systemctl is-enabled --quiet k3s; then
              echo "k3s unit is enabled"
            else
              echo "k3s unit is not enabled (continuing; service may still be running)"
            fi
          fi
      
          # Wait for Kubernetes node to be Ready (longer/retry-safe)
          echo "Waiting for Kubernetes node to be Ready..."
          max_attempts=24
          attempt=1
          while [ $attempt -le $max_attempts ]; do
            if kubectl get nodes --no-headers 2>/dev/null | awk '{print $2}' | grep -q '^Ready$'; then
              echo "Kubernetes node is Ready"
              break
            fi
            echo "Attempt $attempt/$max_attempts: node not Ready yet; sleeping 10s"
            sleep 10
            attempt=$((attempt + 1))
          done
      
          if [ $attempt -gt $max_attempts ]; then
            echo "Nodes did not become Ready in expected time. Dumping logs for debugging:"
            kubectl get pods -A || true
            sudo journalctl -u k3s --no-pager -n 200 || true
            kubectl describe nodes || true
            exit 1
          fi
      
          # Final sanity checks
          kubectl cluster-info || true
          kubectl get nodes -o wide || true

      - name: Configure k3s registry mirrors
        run: |
          sudo mkdir -p /etc/rancher/k3s
          sudo tee /etc/rancher/k3s/registries.yaml <<'EOF'
          mirrors:
            "docker.io":
              endpoint:
                - "https://mirror.gcr.io"
                - "https://harbor.ddlns.net/v2/dolibarr/"
            "registry-1.docker.io":
              endpoint:
                - "https://mirror.gcr.io"
                - "https://harbor.ddlns.net/v2/dolibarr/"
          EOF
          sudo systemctl restart k3s
          kubectl wait --for=condition=Ready nodes --all --timeout=120s

      - name: Install Flux in Kubernetes
        run: flux install --timeout=5m
          
      - name: Verify Flux installation
        run: |
          kubectl -n flux-system wait --for=condition=ready pod --all --timeout=5m
          kubectl -n flux-system get deploy
          kubectl get crds | grep -E "(fluxcd|toolkit)" || true
          kubectl get crd helmreleases.helm.toolkit.fluxcd.io || true

      - name: Install cert-manager
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.0/cert-manager.yaml
          kubectl -n cert-manager wait --for=condition=ready pod --all --timeout=5m

      - name: Create test ClusterIssuer
        run: |
          kubectl apply -f - <<EOF
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt
          spec:
            selfSigned: {}
          EOF

      - name: Deploy 1password
        run: |
          kubectl create ns 1password || true
          kubectl create secret generic onepassword-secret --from-literal=token=${{ secrets.OP_TOKEN }} --namespace=1password || true

      - name: Main branch setup
        run: |
          flux create source git flux-system \
          --url=${{ github.event.repository.html_url }} \
          --branch=main \
          --interval=15m \
          --username=${GITHUB_ACTOR} \
          --password=${{ secrets.GITHUB_TOKEN }} \
          --ignore-paths="clusters/kubenuc-test/flux-system/**" \
          --timeout=60m

          flux create kustomization flux-system \
          --source=flux-system \
          --interval=15m \
          --path=./clusters/kubenuc-test \
          --timeout=60m

      - name: Wait for initial reconciliation
        run: |
          kubectl wait --for=condition=ready pod -l app=source-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=kustomize-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=helm-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=notification-controller -n flux-system --timeout=5m || true
          kubectl -n flux-system wait kustomization/charts --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/apps --for=condition=ready --timeout=30m || true
          echo "Waiting for HR to reconcile..."
          kubectl get hr -A --no-headers | awk '{print $1, $2}' | while read ns name; do
              echo "Waiting for HR: $name"
              kubectl -n "$ns" wait hr "$name" --for=condition=ready --timeout=30m
          done

      - name: Apply feature branch changes
        run: |
          CURRENT_BRANCH="${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}"
          kubectl patch gitrepository flux-system -n flux-system \
            --type merge \
            --patch "{\"spec\":{\"ref\":{\"branch\":\"${CURRENT_BRANCH}\"}}}"
          kubectl wait --for=condition=ready gitrepository/flux-system -n flux-system --timeout=30m

      - name: Verify feature branch reconciliation
        run: |
          flux reconcile source git flux-system --timeout=60m || true
          kubectl -n flux-system wait kustomization/flux-system --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/charts --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/apps --for=condition=ready --timeout=30m || true
          sleep 120
          kubectl get hr -A --no-headers | awk '{print $1, $2}' | while read ns name; do
              echo "Waiting for HR: $name"
              kubectl -n "$ns" wait hr "$name" --for=condition=ready --timeout=30m
          done

      - name: Configure CoreDNS and hosts for test domains
        run: |
          echo "=== Configuring CoreDNS for test domains ==="

          echo "=== Searching for HAProxy ingress service ==="
          # First try to find haproxy-ingress service specifically
          INGRESS_IP=$(kubectl get svc -n haproxy-ingress -o jsonpath='{.items[?(@.metadata.name=="haproxy-ingress")].spec.clusterIP}' 2>/dev/null || echo "")
          
          if [ -n "$INGRESS_IP" ] && [ "$INGRESS_IP" != "null" ]; then
            echo "Found haproxy-ingress service with IP: $INGRESS_IP"
          else
            echo "haproxy-ingress service not found by name, searching by labels..."
            # Try to find any service in haproxy-ingress namespace
            INGRESS_IP=$(kubectl get svc -n haproxy-ingress -o jsonpath='{.items[0].spec.clusterIP}' 2>/dev/null || echo "")
            
            if [ -n "$INGRESS_IP" ] && [ "$INGRESS_IP" != "null" ]; then
              SVC_NAME=$(kubectl get svc -n haproxy-ingress -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
              echo "Found service $SVC_NAME in haproxy-ingress with IP: $INGRESS_IP"
            fi
          fi

          # If still not found, try other namespaces
          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ]; then
            echo "Searching in other ingress namespaces..."
            for NS in ingress-nginx ingress emissary-ingress traefik kong; do
              if kubectl get ns "$NS" &>/dev/null; then
                INGRESS_IP=$(kubectl get svc -n "$NS" -o json | jq -r '
                  .items[] |
                  select(
                    (.spec.type == "LoadBalancer" or .spec.type == "ClusterIP") and
                    (
                      .metadata.labels["app.kubernetes.io/component"] == "controller" or
                      .metadata.labels["app"] == "controller" or
                      .metadata.name | contains("controller") or
                      .metadata.name | contains("ingress")
                    )
                  ) |
                  .spec.clusterIP
                ' | head -1)

                if [ -n "$INGRESS_IP" ] && [ "$INGRESS_IP" != "null" ]; then
                  echo "Found ingress in namespace $NS with IP $INGRESS_IP"
                  break
                fi
              fi
            done
          fi

          # Last resort: use the IP we know works from the curl test
          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ] || [ "$INGRESS_IP" == "None" ]; then
            echo "Could not find ingress service automatically"
            echo "Checking ingress resources for ADDRESS field..."
            INGRESS_IP=$(kubectl get ingress -A -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
            
            if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ]; then
              # Use the ADDRESS from kubectl get ingress
              INGRESS_IP=$(kubectl get ingress -A -o wide | grep -v NAME | awk '{print $4}' | head -1)
            fi
          fi

          echo "=== Final Ingress IP: $INGRESS_IP ==="

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ] || [ "$INGRESS_IP" == "None" ]; then
            echo "ERROR: Could not determine ingress IP!"
            echo "=== Debugging information ==="
            kubectl get svc -A | grep -E "(haproxy|ingress|nginx|traefik)"
            kubectl get ingress -A
            exit 1
          fi

          # Validate IP format
          if ! echo "$INGRESS_IP" | grep -qE '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$'; then
            echo "ERROR: Invalid IP format: $INGRESS_IP"
            exit 1
          fi

          echo "=== Applying CoreDNS configuration with IP: $INGRESS_IP ==="
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: coredns-custom
            namespace: kube-system
          data:
            tst.ddlns.net.override: |
              ${INGRESS_IP} harbor.tst.ddlns.net
              ${INGRESS_IP} jenkins.tst.ddlns.net
              ${INGRESS_IP} artifactory.tst.ddlns.net
              ${INGRESS_IP} cloud.tst.ddlns.net
              ${INGRESS_IP} portainer.tst.ddlns.net
              ${INGRESS_IP} tv.tst.ddlns.net
              ${INGRESS_IP} sso.tst.ddlns.net
              ${INGRESS_IP} s3-api.tst.ddlns.net
              ${INGRESS_IP} nx.s3.tst.ddlns.net
          EOF

          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: coredns
            namespace: kube-system
          data:
            Corefile: |
              .:53 {
                  errors
                  health {
                      lameduck 5s
                  }
                  ready
                  kubernetes cluster.local in-addr.arpa ip6.arpa {
                      pods insecure
                      fallthrough in-addr.arpa ip6.arpa
                      ttl 30
                  }
                  hosts /etc/coredns/custom/tst.ddlns.net.override {
                      fallthrough
                  }
                  prometheus :9153
                  forward . /etc/resolv.conf {
                      max_concurrent 1000
                  }
                  cache 30
                  loop
                  reload
                  loadbalance
              }
          EOF
          
          kubectl -n kube-system patch deployment coredns --type=strategic -p '{"spec":{"template":{"spec":{"volumes":[{"name":"custom-config","configMap":{"name":"coredns-custom","optional":true}}],"containers":[{"name":"coredns","volumeMounts":[{"name":"custom-config","mountPath":"/etc/coredns/custom","readOnly":true}]}]}}}}'
          kubectl -n kube-system rollout restart deployment coredns
          kubectl -n kube-system rollout status deployment coredns --timeout=10m
          
          echo "=== Verifying CoreDNS configuration ==="
          kubectl -n kube-system get configmap coredns-custom -o yaml

      - name: Check resource deployment status
        run: |
          flux get kustomizations -A
          flux get helmreleases -A

      - name: Validate critical deployments
        run: |
          kubectl -n ingress-nginx get pods || true
          kubectl get ingress -A || true
          kubectl -n openebs get pods || true
          kubectl get certificates -A || true
          kubectl get sc || true

      - name: Verify DNS resolution in cluster
        run: |
          echo "=== Testing DNS resolution from a test pod ==="
          kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- sh -c "
            echo 'Testing DNS resolution:'
            nslookup harbor.tst.ddlns.net
            echo ''
            nslookup jenkins.tst.ddlns.net
            echo ''
            nslookup cloud.tst.ddlns.net
          " || echo "DNS test completed"
          
          echo "=== Checking CoreDNS logs ==="
          kubectl -n kube-system logs -l k8s-app=kube-dns --tail=50

      - name: Test ingress connectivity
        run: |
          echo "=== Testing direct connectivity to ingress IP ==="
          kubectl run test-curl --image=curlimages/curl --rm -it --restart=Never -- \
          curl -vk -H "Host: harbor.tst.ddlns.net" https://10.20.0.67/ || true

      - name: Run Robot Framework ingress tests
        run: |
          echo "=== Deploying Robot Framework test job ==="
          kubectl apply -f tests/robot/robot-test-job.yaml

          echo "=== Waiting for pod to be created ==="
          for i in {1..60}; do
            POD_NAME=$(kubectl -n robot-tests get pods -l job-name=robot-ingress-test -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            if [ -n "$POD_NAME" ]; then
              echo "Pod created: $POD_NAME"
              break
            fi
            echo "Waiting for pod to be created... ($i/60)"
            sleep 2
          done

          if [ -z "$POD_NAME" ]; then
            echo "ERROR: Pod was not created"
            exit 1
          fi

          echo "=== Waiting for init container (tests) to complete ==="
          # Wait for pod to have the sidecar container running (which means init container completed)
          kubectl -n robot-tests wait --for=condition=ready pod/$POD_NAME --timeout=10m || {
            echo "Pod did not become ready"
            kubectl -n robot-tests describe pod/$POD_NAME || true
            kubectl -n robot-tests logs pod/$POD_NAME -c robot-test || true
            exit 1
          }

          echo "=== Robot Framework test logs ==="
          kubectl -n robot-tests logs pod/$POD_NAME -c robot-test || {
            echo "Could not retrieve logs from init container"
            exit 1
          }
          
          # Check if tests passed
          if kubectl -n robot-tests logs pod/$POD_NAME -c robot-test | grep -q "| FAIL |"; then
            echo "ERROR: Some tests failed!"
            exit 1
          fi
          
          echo "=== Tests completed successfully ==="

      - name: Extract test results and screenshots
        if: always()
        run: |
          echo "=== Extracting screenshots and results from pod ==="
          POD_NAME=$(kubectl -n robot-tests get pods -l job-name=robot-ingress-test -o jsonpath='{.items[0].metadata.name}')
          echo "Pod name: $POD_NAME"
          
          # Wait for init container to actually complete
          echo "=== Waiting for init container to finish ==="
          for i in {1..120}; do
            INIT_STATUS=$(kubectl -n robot-tests get pod $POD_NAME -o jsonpath='{.status.initContainerStatuses[0].state.terminated.reason}' 2>/dev/null || echo "Running")
            if [ "$INIT_STATUS" = "Completed" ]; then
              echo "Init container completed successfully"
              break
            fi
            if [ "$INIT_STATUS" = "Error" ]; then
              echo "Init container failed!"
              kubectl -n robot-tests logs pod/$POD_NAME -c robot-test || true
              exit 1
            fi
            echo "Waiting for init container... ($i/120) Status: $INIT_STATUS"
            sleep 5
          done
          
          # Check that sidecar is running
          echo "=== Checking sidecar container status ==="
          kubectl -n robot-tests get pod $POD_NAME -o jsonpath='{.status.containerStatuses[0]}' | jq '.' || true
          
          # Wait for sidecar to be ready
          kubectl -n robot-tests wait --for=condition=ready pod/$POD_NAME --timeout=60s || {
            echo "Sidecar not ready"
            kubectl -n robot-tests describe pod $POD_NAME
            exit 1
          }
          
          # Additional wait to ensure filesystem sync
          echo "=== Waiting for filesystem sync ==="
          sleep 10
          
          # List files in pod before copying
          echo "=== Files in pod /results (via exec) ==="
          kubectl -n robot-tests exec $POD_NAME -c results-holder -- sh -c "ls -lah /results/" 2>&1 || {
            echo "Could not exec into pod"
            kubectl -n robot-tests describe pod $POD_NAME
          }
          
          echo "=== Files in pod /results/screenshots (via exec) ==="
          kubectl -n robot-tests exec $POD_NAME -c results-holder -- sh -c "ls -lah /results/screenshots/" 2>&1 || {
            echo "No screenshots directory or cannot access"
          }
          
          echo "=== Count of files in /results ==="
          kubectl -n robot-tests exec $POD_NAME -c results-holder -- sh -c "find /results -type f | wc -l" 2>&1 || echo "Cannot count files"
          
          # Create local directory for results
          mkdir -p robot-results
          
          # Copy results from the running results-holder container
          echo "=== Copying files with kubectl cp ==="
          kubectl -n robot-tests cp ${POD_NAME}:/results/. robot-results/ -c results-holder 2>&1 || {
            echo "Warning: kubectl cp failed"
            echo "Trying alternative method..."
            # Try copying individual files
            kubectl -n robot-tests exec $POD_NAME -c results-holder -- sh -c "tar czf - -C /results ." | tar xzf - -C robot-results/ 2>&1 || {
              echo "Alternative method also failed"
            }
          }
          
          # List what we got
          echo "=== Results extracted locally ==="
          ls -lah robot-results/ 2>&1 || echo "No robot-results directory"
          echo "=== Screenshots locally ==="
          ls -lah robot-results/screenshots/ 2>&1 || echo "No screenshots directory"
          
          # Count files
          FILE_COUNT=$(find robot-results/ -type f 2>/dev/null | wc -l)
          echo "Total files extracted: $FILE_COUNT"
          
          if [ "$FILE_COUNT" -eq 0 ]; then
            echo "ERROR: No files were extracted!"
            echo "Showing pod details for debugging:"
            kubectl -n robot-tests describe pod $POD_NAME
          fi

      - name: Upload Robot Framework results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: robot-test-results
          path: |
            robot-results/
          retention-days: 30

      - name: Cleanup test job
        if: always()
        run: |
          echo "=== Cleaning up test job ==="
          kubectl delete -f tests/robot/robot-test-job.yaml || true

      - name: Check for failed reconciliations
        run: |
          kubectl get kustomizations -A -o json | jq -r '.items[] | select(.status.conditions[]? | select(.type=="Ready" and .status=="False")) | "\(.metadata.namespace)/\(.metadata.name)"' || true
          kubectl get helmreleases -A -o json | jq -r '.items[] | select(.status.conditions[]? | select(.type=="Ready" and .status=="False")) | "\(.metadata.namespace)/\(.metadata.name)"' || true

      - name: Debug failure
        if: failure()
        run: |
          flux get all -A
          kubectl get pods -A | grep -v Running || true
          kubectl get events -A --sort-by='.lastTimestamp' | tail -30 || true

      - name: Cleanup k3s
        if: always()
        run: |
          /usr/local/bin/k3s-uninstall.sh || true
          rm -rf ~/.kube/config || true
          sudo rm -rf /etc/rancher/k3s || true
          sudo rm -rf /var/lib/rancher/k3s || true
          sudo rm -rf /run/k3s || true







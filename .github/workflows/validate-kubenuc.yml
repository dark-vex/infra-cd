name: kubenuc-full-cluster-e2e

on:
  pull_request:
    branches: [main]
    paths:
      - 'clusters/kubenuc/**'

jobs:
  cluster-test:
    runs-on: self-hosted
    timeout-minutes: 120
    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '>=3.11'

      - name: Setup Flux
        uses: fluxcd/flux2/action@main
        with:
          version: 2.7.5

      - name: Setup k3s
        run: |
          set -euo pipefail
      
          # Install k3s
          curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--disable=traefik --write-kubeconfig-mode=644" INSTALL_K3S_VERSION="v1.33.3+k3s1" sh -
          mkdir -p ~/.kube
          sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
          sudo chown $(id -u):$(id -g) ~/.kube/config
      
          # Helper: wait for systemd unit to become active
          wait_for_unit() {
            local unit="$1"
            local timeout="${2:-120}" # seconds
            local interval=5
            local waited=0
      
            if ! command -v systemctl >/dev/null 2>&1; then
              echo "systemctl not present on runner; cannot check systemd unit $unit"
              return 2
            fi
      
            echo "Checking systemd unit: $unit"
      
            # Ensure unit file exists in systemd
            if ! sudo systemctl list-unit-files | grep -q "^${unit}\."; then
              echo "Unit ${unit} not found in systemd unit-files. Continuing to poll in case it appears..."
            fi
      
            while [ $waited -lt "$timeout" ]; do
              # Check whether the unit is active
              if sudo systemctl is-active --quiet "$unit"; then
                echo "Unit ${unit} is active"
                # Also make sure it's not in a failed state
                if sudo systemctl is-failed --quiet "$unit"; then
                  echo "Unit ${unit} is in failed state"
                  return 3
                fi
                return 0
              fi
      
              # Show transient status periodically to aid debugging
              echo "[$((waited + interval))/${timeout}] unit ${unit} not active yet. status:"
              sudo systemctl status "$unit" --no-pager -n 3 || true
      
              sleep $interval
              waited=$((waited + interval))
            done
      
            echo "Timed out waiting for ${unit} to become active (timeout=${timeout}s)."
            echo "Last 200 journal lines for ${unit}:"
            sudo journalctl -u "$unit" --no-pager -n 200 || true
            return 1
          }
      
          # Wait for k3s systemd unit to be active (returns non-zero on failure)
          wait_for_unit k3s 180
      
          # Optional: ensure enabled
          if command -v systemctl >/dev/null 2>&1; then
            if sudo systemctl is-enabled --quiet k3s; then
              echo "k3s unit is enabled"
            else
              echo "k3s unit is not enabled (continuing; service may still be running)"
            fi
          fi
      
          # Wait for Kubernetes node to be Ready (longer/retry-safe)
          echo "Waiting for Kubernetes node to be Ready..."
          max_attempts=24
          attempt=1
          while [ $attempt -le $max_attempts ]; do
            if kubectl get nodes --no-headers 2>/dev/null | awk '{print $2}' | grep -q '^Ready$'; then
              echo "Kubernetes node is Ready"
              break
            fi
            echo "Attempt $attempt/$max_attempts: node not Ready yet; sleeping 10s"
            sleep 10
            attempt=$((attempt + 1))
          done
      
          if [ $attempt -gt $max_attempts ]; then
            echo "Nodes did not become Ready in expected time. Dumping logs for debugging:"
            kubectl get pods -A || true
            sudo journalctl -u k3s --no-pager -n 200 || true
            kubectl describe nodes || true
            exit 1
          fi
      
          # Final sanity checks
          kubectl cluster-info || true
          kubectl get nodes -o wide || true

      - name: Configure k3s registry mirrors
        run: |
          sudo mkdir -p /etc/rancher/k3s
          sudo tee /etc/rancher/k3s/registries.yaml <<'EOF'
          mirrors:
            "docker.io":
              endpoint:
                - "https://mirror.gcr.io"
                - "https://harbor.ddlns.net/v2/dolibarr/"
            "registry-1.docker.io":
              endpoint:
                - "https://mirror.gcr.io"
                - "https://harbor.ddlns.net/v2/dolibarr/"
          EOF
          sudo systemctl restart k3s
          kubectl wait --for=condition=Ready nodes --all --timeout=120s

      - name: Install Flux in Kubernetes
        run: flux install --timeout=5m
          
      - name: Verify Flux installation
        run: |
          kubectl -n flux-system wait --for=condition=ready pod --all --timeout=5m
          kubectl -n flux-system get deploy
          kubectl get crds | grep -E "(fluxcd|toolkit)" || true
          kubectl get crd helmreleases.helm.toolkit.fluxcd.io || true

      - name: Install cert-manager
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.0/cert-manager.yaml
          kubectl -n cert-manager wait --for=condition=ready pod --all --timeout=5m

      - name: Create test ClusterIssuer
        run: |
          kubectl apply -f - <<EOF
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt
          spec:
            selfSigned: {}
          EOF

      - name: Deploy 1password
        run: |
          kubectl create ns 1password || true
          kubectl create secret generic onepassword-secret --from-literal=token=${{ secrets.OP_TOKEN }} --namespace=1password || true

      - name: Main branch setup
        run: |
          flux create source git flux-system \
          --url=${{ github.event.repository.html_url }} \
          --branch=main \
          --interval=15m \
          --username=${GITHUB_ACTOR} \
          --password=${{ secrets.GITHUB_TOKEN }} \
          --ignore-paths="clusters/kubenuc-test/flux-system/**" \
          --timeout=60m

          flux create kustomization flux-system \
          --source=flux-system \
          --interval=15m \
          --path=./clusters/kubenuc-test \
          --timeout=60m

      - name: Wait for initial reconciliation
        run: |
          kubectl wait --for=condition=ready pod -l app=source-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=kustomize-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=helm-controller -n flux-system --timeout=5m || true
          kubectl wait --for=condition=ready pod -l app=notification-controller -n flux-system --timeout=5m || true
          kubectl -n flux-system wait kustomization/charts --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/apps --for=condition=ready --timeout=30m || true
          echo "Waiting for HR to reconcile..."
          kubectl get hr -A --no-headers | awk '{print $1, $2}' | while read ns name; do
              echo "Waiting for HR: $name"
              kubectl -n "$ns" wait hr "$name" --for=condition=ready --timeout=30m
          done

      - name: Apply feature branch changes
        run: |
          CURRENT_BRANCH="${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}"
          kubectl patch gitrepository flux-system -n flux-system \
            --type merge \
            --patch "{\"spec\":{\"ref\":{\"branch\":\"${CURRENT_BRANCH}\"}}}"
          kubectl wait --for=condition=ready gitrepository/flux-system -n flux-system --timeout=30m

      - name: Verify feature branch reconciliation
        run: |
          flux reconcile source git flux-system --timeout=60m || true
          kubectl -n flux-system wait kustomization/flux-system --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/charts --for=condition=ready --timeout=10m || true
          kubectl -n flux-system wait kustomization/apps --for=condition=ready --timeout=30m || true
          sleep 120
          kubectl get hr -A --no-headers | awk '{print $1, $2}' | while read ns name; do
              echo "Waiting for HR: $name"
              kubectl -n "$ns" wait hr "$name" --for=condition=ready --timeout=30m
          done

      - name: Configure CoreDNS and hosts for test domains
        run: |
          echo "=== Configuring CoreDNS for test domains ==="

          DEFAULT_INGRESS_CLASS=$(kubectl get ingressclass -o jsonpath='{.items[?(@.metadata.annotations.ingressclass\.kubernetes\.io/is-default-class=="true")].metadata.name}' 2>/dev/null || echo "")

          if [ -n "$DEFAULT_INGRESS_CLASS" ]; then
            echo "Found default IngressClass: $DEFAULT_INGRESS_CLASS"
          fi

          INGRESS_IP=""

          for NS in ingress-nginx ingress emissary-ingress traefik haproxy-ingress kong; do
            if kubectl get ns "$NS" &>/dev/null; then
              SVC=$(kubectl -n "$NS" get svc -o json | jq -r '
                .items[] |
                select(
                  (.spec.type == "LoadBalancer" or .spec.type == "ClusterIP") and
                  (
                    .metadata.labels["app.kubernetes.io/component"] == "controller" or
                    .metadata.labels["app"] == "controller" or
                    .metadata.name | contains("controller") or
                    .metadata.name | contains("ingress")
                  )
                ) |
                "\(.metadata.name) \(.spec.clusterIP)"
              ' | head -1)

              if [ -n "$SVC" ]; then
                SVC_NAME=$(echo "$SVC" | awk '{print $1}')
                INGRESS_IP=$(echo "$SVC" | awk '{print $2}')
                echo "Found ingress service: $SVC_NAME in namespace $NS with IP $INGRESS_IP"
                break
              fi
            fi
          done

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ]; then
            echo "Trying fallback: looking for any service exposing port 80/443..."
            INGRESS_IP=$(kubectl get svc -A -o json | jq -r '
              .items[] |
              select(
                .spec.ports[]? |
                select(.port == 80 or .port == 443 or .targetPort == 80 or .targetPort == 443)
              ) |
              select(.metadata.namespace != "default" and .metadata.namespace != "kube-system") |
              .spec.clusterIP
            ' | grep -v null | head -1)
          fi

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ] || [ "$INGRESS_IP" == "None" ]; then
            echo "Using k3s node IP as fallback..."
            INGRESS_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
          fi

          echo "Final Ingress IP: $INGRESS_IP"

          if [ -z "$INGRESS_IP" ] || [ "$INGRESS_IP" == "null" ]; then
            echo "Could not determine ingress IP, using placeholder"
            INGRESS_IP="10.96.0.100"
          fi

          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: coredns-custom
            namespace: kube-system
          data:
            tst.ddlns.net.override: |
              ${INGRESS_IP} harbor.tst.ddlns.net
              ${INGRESS_IP} jenkins.tst.ddlns.net
              ${INGRESS_IP} artifactory.tst.ddlns.net
              ${INGRESS_IP} cloud.tst.ddlns.net
              ${INGRESS_IP} portainer.tst.ddlns.net
              ${INGRESS_IP} tv.tst.ddlns.net
              ${INGRESS_IP} sso.tst.ddlns.net
              ${INGRESS_IP} s3-api.tst.ddlns.net
              ${INGRESS_IP} nx.s3.tst.ddlns.net
          EOF

          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: coredns
            namespace: kube-system
          data:
            Corefile: |
              .:53 {
                  errors
                  health {
                      lameduck 5s
                  }
                  ready
                  kubernetes cluster.local in-addr.arpa ip6.arpa {
                      pods insecure
                      fallthrough in-addr.arpa ip6.arpa
                      ttl 30
                  }
                  hosts /etc/coredns/custom/tst.ddlns.net.override {
                      fallthrough
                  }
                  prometheus :9153
                  forward . /etc/resolv.conf {
                      max_concurrent 1000
                  }
                  cache 30
                  loop
                  reload
                  loadbalance
              }
          EOF
          kubectl -n kube-system patch deployment coredns --type=strategic -p '{"spec":{"template":{"spec":{"volumes":[{"name":"custom-config","configMap":{"name":"coredns-custom","optional":true}}],"containers":[{"name":"coredns","volumeMounts":[{"name":"custom-config","mountPath":"/etc/coredns/custom","readOnly":true}]}]}}}}'
          kubectl -n kube-system rollout restart deployment coredns
          kubectl -n kube-system rollout status deployment coredns --timeout=10m

      - name: Check resource deployment status
        run: |
          flux get kustomizations -A
          flux get helmreleases -A

      - name: Validate critical deployments
        run: |
          kubectl -n ingress-nginx get pods || true
          kubectl get ingress -A || true
          kubectl -n openebs get pods || true
          kubectl get certificates -A || true
          kubectl get sc || true

      - name: Run Robot Framework ingress tests
        run: |
          echo "=== Deploying Robot Framework test job ==="
          kubectl apply -f tests/robot/robot-test-job.yaml

          echo "=== Waiting for job to complete ==="
          kubectl -n robot-tests wait --for=condition=complete --timeout=10m job/robot-ingress-test || {
            echo "Job did not complete successfully, checking logs..."
            kubectl -n robot-tests logs job/robot-ingress-test
            exit 1
          }

          echo "=== Robot Framework test logs ==="
          kubectl -n robot-tests logs job/robot-ingress-test

      - name: Extract test results and screenshots
        if: always()
        run: |
          echo "=== Extracting screenshots and results from pod ==="
          POD_NAME=$(kubectl -n robot-tests get pods -l job-name=robot-ingress-test -o jsonpath='{.items[0].metadata.name}')
          echo "Pod name: $POD_NAME"
          
          # Create local directory for results
          mkdir -p robot-results/screenshots
          
          # Copy results from pod
          kubectl -n robot-tests cp ${POD_NAME}:/results/. robot-results/ || {
            echo "Warning: Could not copy all results"
          }
          
          # List what we got
          echo "=== Results extracted ==="
          ls -lah robot-results/ || true
          ls -lah robot-results/screenshots/ || true

      - name: Upload Robot Framework results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: robot-test-results
          path: |
            robot-results/
          retention-days: 30

      - name: Cleanup test job
        if: always()
        run: |
          echo "=== Cleaning up test job ==="
          kubectl delete -f tests/robot/robot-test-job.yaml || true

      - name: Check for failed reconciliations
        run: |
          kubectl get kustomizations -A -o json | jq -r '.items[] | select(.status.conditions[]? | select(.type=="Ready" and .status=="False")) | "\(.metadata.namespace)/\(.metadata.name)"' || true
          kubectl get helmreleases -A -o json | jq -r '.items[] | select(.status.conditions[]? | select(.type=="Ready" and .status=="False")) | "\(.metadata.namespace)/\(.metadata.name)"' || true

      - name: Debug failure
        if: failure()
        run: |
          flux get all -A
          kubectl get pods -A | grep -v Running || true
          kubectl get events -A --sort-by='.lastTimestamp' | tail -30 || true

      - name: Cleanup k3s
        if: always()
        run: |
          /usr/local/bin/k3s-uninstall.sh || true
          rm -rf ~/.kube/config || true
          sudo rm -rf /etc/rancher/k3s || true
          sudo rm -rf /var/lib/rancher/k3s || true
          sudo rm -rf /run/k3s || true

